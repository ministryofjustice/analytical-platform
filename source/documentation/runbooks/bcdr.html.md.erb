---
owner_slack: "#analytical-platform-notifications"
title: Business continuity and disaster recovery
last_reviewed_on: 2025-12-12
review_in: 6 months
weight: 0
---

# <%= current_page.data.title %>

## Overview

This document outlines the business continuity and disaster recovery (BCDR) processes for the Analytical Platform, as well as our appetite for risk in relation to the uptime and recoverability of the service. These processes could be invoked by infrastructure failure, cyber attacks or human error.

## Steps to take

If an incident which requires full or partial platform recovery occurs, follow these steps:

- Communicate the issue - see the [section below](#communication-plan)
- Identify the cause
- Agree on recovery action(s)
- Update communications
- Perform recovery actions (or monitor external incident sources - see below)
- Verify that operations are restored
- Update communications
- Perform a post-incident review, identify actions and update runbooks if applicable (this can be done later)

### Issues with underlying platforms

If the cause is identified as being an incident or issue with Modernisation Platform (or Cloud Platform for user-deployed apps) then recovery will largely be out of the hands of this team. Monitor the [#ask-moderinsation-platform] (or [#ask-cloud-platform]) channel and [relay any relevant information](#communication-plan) to users.

If the issue is with underlying AWS infrastructure (an AWS outage) then monitor the relevant AWS incident page and relay information as above.

Once the underlying issue is resolved, perform manual checks of the availability of AP services before communicating the resolution to users.

## Communication plan

- **Internal Communication**:
  - Notify the Analytical Platform team immediately when a incident is identified via the [#analytical-platform-slack] Slack channel.
  - Create an incident thread in the above channel and tag the team using the `@analytical-platform-team` tag (not `@here` or `@channel`)
- **External Communication**:
  - Inform users about any relevant service disruptions via the [#ask-analytical-plaform] Slack channel and user groups such as the #ask-data-engineering Slack channel and DEDS email group.

## Business appetite for time to recovery

While we offer 24/7 availability, the Analytical Platform is delivered as a "business hours" service - we do not offer out of hours support.

The AP was not orginally developed as a business critical service, and any business criticality has evolved as a consequence of way it has been used.

Consequently, our target recovery time for platform availability is **five working days**.

It is important to note that "platform availability" is not the same as a full platform recovery. While core functionality will be restored, this will be a barebones recreation of the AP. Recovery of data, user roles and access to data would will take considerably longer and full restoration may not be possible in all cases.

### Consequences of platform unavailability

- Potential delays to published statistics
  - The outputs (ODS / CSV) for these will generally have been generated ahead of publication date for signoff and scheduling purposes
- Potential delays in answering PQs or FOI requests
  - These will generally have a reasonable SLA for turnaround
- Potential delays to internal publications such as departmental dashboards, prison or probation performance packs
- Delays to data updates of user-deployed apps (R-Shiny), or consequential downtime or degradation for those apps
- Delays to data processing for onward consumption by other services
- Faults in applications dependent on platform-hosted APIs (for example LLMs)
- Loss of access to hosted development environments, leading to delays in delivering work

## Data loss

In general data is owned by users of the platform, and we do not currently back up or replicate user data held in S3 buckets. In the event of data loss, data must be re-ingested or re-uploaded from the original sources.

We rely on the reliability of AWS S3 standard storage:

- Backed with the [Amazon S3 Service Level Agreement](https://aws.amazon.com/s3/sla/).
- Designed to provide 99.999999999% durability and 99.99% availability of objects over a given year.
- S3 Standard (... etc.) are all designed to sustain data in the event of the loss of an entire Amazon S3 Availability Zone.

[Source](https://docs.aws.amazon.com/AmazonS3/latest/userguide/DataDurability.html).

Given the above we do not consider it cost effective to replicate or otherwise back up petabytes of data.

We do not guarantee protection against accidental deletion or modification of data by users. Accidental deletion does not constitute an incident or disaster in the context of this documument.

For context, the AP was not orignally designed as a primary data store - it is a secondary store of data which exists elsewhere. If apps have been developed which collect data, these should have their own processes for ensuring business continuity.

## Platform recovery

The AP contains some technical debt which makes committing to an exact recovery plan difficult. In early iterations of the platform, some resouces (infrastructure, roles etc.) were created in the AWS console ("clickops").

That said, much of the infrastructure is defined in code, with the core of the platform being defined in Terraform in the main Analytical Platform repository, and newer infrastructure defined entirely in code in the Modernisation Platform environments repository.

### List of repositories where core infrastructure is defined

- [Analytical Platform main repo](https://github.com/ministryofjustice/analytical-platform)
- [Common](https://github.com/ministryofjustice/modernisation-platform-environments/tree/main/terraform/environments/analytical-platform-common)
- [Compute](https://github.com/ministryofjustice/modernisation-platform-environments/tree/main/terraform/environments/analytical-platform-compute)
- [Ingestion](https://github.com/ministryofjustice/modernisation-platform-environments/tree/main/terraform/environments/analytical-platform-ingestion)

Disaster recovery is effectively re-running infrastructure as code to recreate the AWS resources which comprise the platform. At the time of writing, we have not simulated a recovery, which means the recovery plan follows an iterative process.

## Recovery plan

1. Identify stages or platform components for partial recovery (identify order of operations for running Terraform)
2. Run Terraform for the first platform component(s)
3. Run manual checks of restored resources, and attempt to identify missing resources which were created manually
4. Update infrastructure code with those resources
5. Apply infrastructure code
6. Run manual tests
7. Iterate from step 3 for the next logical component

The above will result in a recovered, operational platform. The next steps are:

- Restore user accounts - these are based on GitHub users, so are separate from the platform itself
- Restore ETL jobs (see [section below](#etl-airflow))
- Re-run Airflow jobs - this could take several days
- Restore and re-run CaDeT jobs (see [section below](#create-a-derived-table-cadet)) - this could also take several days
- Restore user data access permissions (see [section below](#data-access))
- Restore Control Panel (see [section below](#control-panel))

When restoring pipelines, derived data jobs, user access, and the Control Panel database, it is possible that S3 paths and IAM role ARNs could have re-created. In this scenario, restoration would require re-mapping of the equivalent paths and roles.

### ETL (Airflow)

After core infrastructure has been recovered, data pipelines to the AP can be rebuilt from the [airflow-repo]. All jobs are defined in code.

### Create a Derived Table (CaDeT)

[CaDeT] is based on [dbt](https://www.getdbt.com/) where all workflows and derived data defintions are defined in code. Once source data has been re-ingested the models can be updated to reflect any changed S3 paths and IAM role ARNs and re-run.

### Data access

User access to data is defined in code in the data engineering database access repo: [data-access-repo].

### Control Panel

Control Panel ([repository](https://github.com/ministryofjustice/analytics-platform-control-panel)) is a Python-based web application with an RDS data store. Once the infrastructure is restored, recovery of Control Panel would comprise restoring the database from the latest snapshot, and re-deploying the application using its GitHub action.

<!-- external links -->
[#ask-analytical-platform] : https://moj.enterprise.slack.com/archives/C4PF7QAJZ
[#analytical-platform-slack] : https://moj.enterprise.slack.com/archives/C04M8224WCV
[#modernisation-platform-update] : https://moj.enterprise.slack.com/archives/C02L5MCJ12N
[#ask-moderinsation-platform]: https://moj.enterprise.slack.com/archives/C57UPMZLY
[#cloud-platform-update] : https://moj.enterprise.slack.com/archives/CH6D099DF
[#ask-cloud-platform]: https://moj.enterprise.slack.com/archives/C57UPMZLY
[#ap-repo]: https://github.com/ministryofjustice/analytical-platform
[CaDeT] : https://github.com/moj-analytical-services/create-a-derived-table/tree/main
[airflow-repo] : https://github.com/ministryofjustice/analytical-platform-airflow
[data-access-repo]: https://github.com/moj-analytical-services/data-engineering-database-access