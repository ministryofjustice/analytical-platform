---
owner_slack: "#analytical-platform-notifications"
title: Business continuity and disaster recovery
last_reviewed_on: 2025-12-10
review_in: 6 months
weight: 0
---

# <%= current_page.data.title %>

## Overview

This document outlines the business continuity and disaster recovery (BCDR) processes for the Analytical Platform, as well as our appetite for risk in relation to the uptime and recoverability of the service. These processes could be invoked by infrastructure failure, cyber attacks or human error.

## Steps to take

If an incident which requires full or partial platform recovery occurs, follow these steps:

- Communicate the issue - see the [section below](#communication-plan)
- Identify the cause
- Agree on recovery action(s)
- Update communications
- Perform recovery actions (or monitor external incident sources - see below)
- Verify that operations are restored
- Update communications
- Perform a post-incident review, identify actions and update runbooks if applicable (this can be done later)

### Issues with underlying platforms

If the cause is identified as being an incident or issue with Modernisation Platform (or Cloud Platform for user-deployed apps) then recovery will largely be out of the hands of this team. Monitor the [#ask-moderinsation-platform] (or [#ask-cloud-platform]) channel and [relay any relevant information](#communication-plan) to users.

If the issue is with underlying AWS infrastructure (an AWS outage) then monitor the relevant AWS incident page and relay information as above.

Once the underlying issue is resolved, perform manual checks of the availability of AP services before communicating the resolution to users.

## Communication plan

- **Internal Communication**:
  - Notify the Analytical Platform team immediately when a incident is identified via the [#analytical-platform-slack] Slack channel.
  - Create an incident thread in the above channel and tag the team using the `@analytical-platform-team` tag (not `@here` or `@channel`)
- **External Communication**:
  - Inform users about any relevant service disruptions via the [#ask-analytical-plaform] Slack channel and user groups such as the #ask-data-engineering Slack channel and DEDS email group.

## Business appetite for time to recovery

While we offer 24/7 availability, the Analytical Platform is delivered as a "business hours" service - we do not offer out of hours support. In that context, we do not currently consider the platform **as a whole** to be a business-critical service, in the same way as strategic hosting or core case management applications would be. However we are aware that certain user-developed processes and apps on the AP *may* be considered business critical.

Consequently, our target recovery time for platform unavailability is **five working days**.
*for a barebones recreation of the AP but not roles, paths etc.*

### Consequences of platform unavailability

- Potential delays to published statistics
  - The outputs (ODS / CSV) for these will generally have been generated ahead of publication date for signoff and scheduling purposes
- Potential delays in answering PQs or FOI requests
  - These will generally have a reasonable SLA for turnaround
- Potential delays to internal publications such as departmental dashboards, prison or probation performance packs
- Delays to data updates of user-deployed apps (R-Shiny), or consequential downtime or degradation for those apps
- Delays to data processing for onward consumption by other services
- Faults in applications dependent on platform-hosted APIs (for example LLMs)
- Loss of access to hosted development environments, leading to delays in delivering work

## Data loss

In general data is owned by users of the platform, and we do not currently back up or replicate user data held in S3 buckets. In the event of data loss, data must be re-ingested or re-uploaded from the original sources.

We rely on the reliability of AWS S3 standard storage:

- Backed with the [Amazon S3 Service Level Agreement](https://aws.amazon.com/s3/sla/).
- Designed to provide 99.999999999% durability and 99.99% availability of objects over a given year.
- S3 Standard (... etc.) are all designed to sustain data in the event of the loss of an entire Amazon S3 Availability Zone.

[Source](https://docs.aws.amazon.com/AmazonS3/latest/userguide/DataDurability.html).

Given the above we do not consider it cost effective to replicate or otherwise back up petabytes of data.

We do not guarantee protection against accidental deletion or modification of data by users. Accidental deletion does not constitute an incident or disaster in the context of this documument.

## Platform recovery

The AP contains some technical debt which makes committing to an exact recovery plan difficult. In early iterations of the platform, some resouces (infrastructure, roles etc.) were created in the AWS console ("click ops").

That said, much of the infrastructure is defined in code, with the core of the platform being defined in Terraform in the main Analytical Platform repository, and newer infrastructure defined entirely in code in the Modernisation Platform environments repository.

### List of repositories where core infrastructure is defined

- [Analytical Platform main repo](https://github.com/ministryofjustice/analytical-platform)
- [Common](https://github.com/ministryofjustice/modernisation-platform-environments/tree/main/terraform/environments/analytical-platform-common)
- [Compute](https://github.com/ministryofjustice/modernisation-platform-environments/tree/main/terraform/environments/analytical-platform-compute)
- [Ingestion](https://github.com/ministryofjustice/modernisation-platform-environments/tree/main/terraform/environments/analytical-platform-ingestion)

Disaster recovery is effectively re-running infrastructure as code to recreate the AWS resources which comprise the platform. At the time of writing, we have not been able to simulate a recovery, which means the recovery plan follows an iterative process.

### Recovery plan

1. Identify stages or platform components for partial recovery (identify order of operations for running Terraform)
2. Run Terraform for the first platform components
3. Run manual checks of restored resources, and attempt to identify missing resources which were created manually
4. Update infrastructure code with those resources
5. Apply infrastructure code
6. Run manual tests
7. Iterate from step 3 for the next logical component

### Data access

*Data engineering data access repo can re-create project access, but the S3 paths could be different. IAM ARNS?*

### Register My Data

*Re-create - as above; paths and IAM roles ARNs could change*

### Airflow and managed pipelines

*Something here from JHP*

Rebuild from [airflow-repo]

### Create a Derived Table (CaDeT)

[CaDeT] is based on [dbt](https://www.getdbt.com/) where all workflows and derived data defintions are defined in code. Once source data has been re-ingested the models can be updated to reflect any changed S3 paths and IAM role ARNs and re-run.


as a consequence, a full restoration of the state of the AP may take months or be impossible


### Tooling

*Recovery of CP and tooling - tooling will be rebuilt as part of the infrastructre; CP re-deployed from its GHA*

CP RDS from snapshot, again S3 path and and role ARNs

<!-- external links -->
[#ask-analytical-platform] : https://moj.enterprise.slack.com/archives/C4PF7QAJZ
[#analytical-platform-slack] : https://moj.enterprise.slack.com/archives/C04M8224WCV
[#modernisation-platform-update] : https://moj.enterprise.slack.com/archives/C02L5MCJ12N
[#ask-moderinsation-platform]: https://moj.enterprise.slack.com/archives/C57UPMZLY
[#cloud-platform-update] : https://moj.enterprise.slack.com/archives/CH6D099DF
[#ask-cloud-platform]: https://moj.enterprise.slack.com/archives/C57UPMZLY
[#ap-repo]: https://github.com/ministryofjustice/analytical-platform
[CaDeT] : https://github.com/moj-analytical-services/create-a-derived-table/tree/main
[airflow-repo] : https://github.com/ministryofjustice/analytical-platform-airflow

<!-- Log retenion - MP default 365 days. S3 object logging. -->