---
owner_slack: "#analytical-platform"
title: CaDeT Support - Tips and tricks
last_reviewed_on: 2024-08-12
review_in: 6 months
---

# <%= current_page.data.title %>

This is here to help you get to the bottom of what's wrong in any given CaDeT Support Issue. This will take you through what information you need from the customer, what information you need to gather behind the scenes yourself, to know how to help any given customer.

## Table of Contents

1. [Quickstart Guide](#quickstart-guide)
2. [What to do?](#what-to-do)
3. [Advanced Troubleshooting](#advanced-troubleshooting)
4. [Contact Information](#contact-information)


## Quickstart Guide

This aims to break all support requests for the product down into discrete categories. Once you know what category your support request comes under, you'll know how roughly to tackle it. Later on, we'll look at an example from each of these categories in more detail.

### Source Updates

Weekly PR (raised on a Monday) by a bot. The PR it raises will always be called `Update Dbt sources`, and will contain a bunch of fairly arcane looking changes to a number of YAML files. The robot is just scraping the glue catalog for info, so you don't need to worry too much about the contents of these. We generally post about this in #data-modelling first thing on a Monday, to let them object if there's any changes they're not set up to cope with. If the PR has literally any title other than this, it's not this category.

A user may approach us asking for an unscheduled source update - in that case, you simply run the dispatch workflow on [this github action](https://github.com/moj-analytical-services/create-a-derived-table/actions/workflows/update-source.yml), and look for the PR it generates. From there, it's as above.

### Deploying New Tables/Databases

Generally, the development and deployment of new tables/databases is the domain of the data modelling team - review of any model files should be left for them to do. However, a key element of this deployment of a new database will require them to modify the [dbt project file](https://github.com/moj-analytical-services/create-a-derived-table/blob/2f3a8d32ded317cd2f3d94329824ba80fe5b0593/mojap_derived_tables/dbt_project.yml#L52), which we reserve control over, to add a deployment schedule. Your role in reviewing the PR is to ensure any new databases have an associated deployment schedule that matches one of our tags (`daily`, `weekly`, `monthly` etc.), and to ensure that they've sought data modeller review for all other elements before merging.

### Changes to workflows/profiles

Sometimes, users will need to do new types of deployment, or will need new targets specific to them to deploy to. We would generally want to take on this work via a feature request. Should this come in via support channels, your role is to steer them towards the 'Create-A-Derived-Table Cross Team Meeting' which occurs fortnightly on Thursday afternoons via Teams.


### Workflow Failure

If a workflow has failed in a way that a user is approaching the team about, it's almost certainly not due to their SQL failing. These are characterised by the users raising bug reports about the action runs themselves. These are generally the hardest category to diagnose, as these failures can occur for a number of reasons and each has a fairly distinct solution.


## What to do?

Having categorised your problem, you now need to figure out how to solve it. If you're already familar with support, hopefully the quickstart guide has reminded you what to do, but if you need more help you're in the right place! Thankfully, for most of these requests, there's very clear steps to how we'd want to handle them.

### Source Updates - In Detail

DBT, the software powering all our table transformation, has two ways of referencing a table. Either the table is created from within DBT, in which case it is a `model`, or it is a table that exists externally to DBT - a `source`. We have various sources from within the organisation, mostly existing datasets that need to be transformed into nicely modeled outputs. Our sources are mangaged [via a github actions workflow](https://github.com/moj-analytical-services/create-a-derived-table/actions/workflows/update-source.yml), which when run successfully will generate [a corresponding PR (example)](https://github.com/moj-analytical-services/create-a-derived-table/pull/1912/files). This may look very intimidating, but since the bot is just scraping data, you do not need to validate the output yourself directly.

**Step by Step:**

1. If a user requests a source update, run the `dispatch_workflow` option on the linked workflow. Wait for it to spit out a PR. Otherwise, it's Monday morning and the bot has run and made the PR for you
2. Check the PR for any big changes - 'big' in this context usually means either a diff too big for github to render by default, or any files added or removed. If they exist, note which files they pertain to.
3. Make a post in #data-modelling with a link to the PR generated. Note any added or removed databases in your post.
4. Allow a couple hours for objections to be raised against the PR. If there are none, approve and merge it.

### Deploying New Tables/Databases - In Detail

The AP team sits as maintainers of the tool - we ensure it stays functional and supply new features. However, we do not exist to advise users on the practice or theory of Data Modelling itself. As such, we generally will not be involved with the reviewing of model code, leaving this to our friends in the Data Modelling function. To look at an example PR to see what files you should/should not care about:

![Example PR Files](cadet-PR-example.png)

In the example here, we only care about the top two files (`dbt_project.yml` and `sop_staging.yml`). These two, combined, tell us that the user is deploying a new database (and some number of tables within). We do not need to review the SQL files, or any associated metadata files, beyond checking with the user that a data modeller has signed off on them. Your role in reviewing the PR is to check that for each unique `database_name.yml` file listed in the added files, there's a corresponding entry in `dbt_project.yml` with a `+tags: <schedule_argument>`. Standard schedule tags are `daily`, `weekly`, `monthly`, and `paused`, but new databases may not use `paused` tag unless they are changing it from one of `daily`, `weekly`, or `monthly` in the PR (I.E. The table has been deployed at least once before pausing). This is important as all tables in main MUST deploy at least once, ideally as soon as possible after merge into main (using the `daily` tag), otherwise it will cause the `generate dbt docs` workflow to fail daily until the database is either removed from main, or deployed. This will cause catalog data to become stale. Some projects (e.g. BOLD) may use their own scheduling tags (`bold_dev_daily` etc.). This is also acceptable, as long as the tag and the domain (folder) that the table exists in are aligned - Only the BOLD domain should use bold related tags and so on.

**Step by Step:**

1. Check whether the PR features one or more added files in the `mojap_derived_tables/models/...` directory. If it does, check if any of those are `.yml` files.
2. Assuming all files are `SQL` files, you're done! It's for the data modellers to review. Otherwise...
3. For each `database_name.yml` file added, ensure a corresponding change is made to `dbt_project.yml` to give it a schedule of `daily` (if they've picked another schedule, check with the user that the table will be deployed immediately on whatever their schedule tag is)
4. If the user has created a schedule for all their new databases/tables, then you're good to approve!

### Changes to Profiles/Workflows

[The profiles file](https://github.com/moj-analytical-services/create-a-derived-table/blob/main/.dbt/profiles.yml) is used to define the (currently as of writing) 4 targets that users can deploy to. These represent our environments, and all targets other than `prod` will cause the generated databases to be suffixed with `_<env>_dbt`. The targets themselves, `dev`, `prod`, `sandpit`, and `sandbox`, all represent different S3 locations that tables can be deployed against, with different access requirements. Very occasionally, a user will want a new target - if they do, make sure they have routed this request via the 'Create-A-Derived-Table Cross Team Meeting' which occurs fortnightly on Thursday afternoons via Teams. If they have, a feature request should have been raised pertaining to this work - link them to the ticket and let them know the work is planned. If they've not routed this via this group, ensure they do.

This file is consumed by our Github Actions Workflows when they deploy tables - all workflows published should generally supply `--target prod`, when running `dbt` commands - they should have a good reason why not if they don't. If a user wants to establish a new deployment workflow, again ensure requests are routed via the cross team meeting. This will allow them to properly be reviewed by the team.

**Step by Step:**

1. Check the proposed changes have been routed via the correct working group
2. If they have, consult the ticket capturing the proposed changes to ensure the PR aligns with the proposal.
3. If so, approve. Otherwise, ensure they go via the correct group.

### Workflow Failures

We have two groups of workflows that commonly fail - `deploy dbt docs`, and our database deployments. To start with `deploy docs`:

#### Deploy DBT Docs

This workflow scrapes the glue catalog for the information about all database within the project, so full lineage of all tables (including source tables) can be constructed. The reason this fails 99.9% of the time is due to a 'missing' database. A missing database is any database that `DBT` expects to encounter (based on the state of main as of the action run), but fails to find when querying glue. If this is a database that is listed in `mojap_derived_tables/models/sources/`, this means somebody blew up a prod table that we use as a source. Generally, that's not on us to solve - simply make a post in `#ask-data-modelling` naming the missing database (as per the error in the action run logs) and asking what's happened to it.

If the table referenced shows up anywhere else in the `models` folder, that means it's generated from within the tool. This means that somebody has merged a table into main that hasn't been included in any deployments yet. Hopefully, this will have been caught at the PR stage (as per above), but this isn't guaranteed. In the event this happens, offer the user two options. Either they can:
- Remove the database from main and keep it in a branch until such time as they're ready to deploy it
OR
- Deploy the database by setting the schedule tag to `daily`, and then reverting it to the generally intended deployment schedule the day after the intial deployment.
Either way, it's important that non-deployed tables don't persist in main, as `generate dbt docs` will fail every day unless all tables in main are (or will be overnight) deployed tables (there is a ticket in the backlog to enforce first deployments being on daily schedule, but this is currently not programatically validated).

**Step by Step:**
1. Find the named database from the workflow's logs in CaDeT's code, and detemine whether it is a `source` or a general `model` in any other domain.
2. If it is a `source` make a post in #ask-data-modelling highlighting the issue - if the owner indicates it might take a long time to resolve, consider removing the database from the [source databases file](https://github.com/moj-analytical-services/create-a-derived-table/blob/main/scripts/source_database_names.txt) and re-running `update source` to remove the database as a source until it's restored.
3. Otherwise, for models, make a post in the same channel, highlighting that the database is not currently deployed. Ensure the user either removes their models from main to a branch, or uses the `daily` tag to deploy the model overnight to ensure the next day's workflow run is a success.

#### General Deployment Failures

These are places where one of our database deploying workflows (such as `deploy_prod_daily`), has failed. When it does so, there's a few ways this can happen.
- Someone submitted SQL that passed in `dev` but failed in `prod` (rare, but not impossible)
- The pod has finished its workflow, but a table or tables have failed as part of that for non-SQL reasons
- The pod has exited without finishing the workflow, and steps after the main deployment are skipped.

To look at each in turn:
**Bad SQL:**
This will be characterised by the errors thrown at the bottom of the `Deploy dbt models` step's logs - this will be the step that failed in the action run in this case. The errors seen will relate to the `StartQueryExecution` action, and will reference an `invalid query` or `bad syntax` etc. If the workflow fails in this manner, the user will generally be notified, and this is not for us to proactively solve - SQL issues are for data owners.

**Boto Looking Errors (Non-SQL):**
These will look similar to the above, with the action completing in full, and the failure being seen in the `Deploy DBT models` step. However, the error messages this time will not generally reference Query Execution, but instead may reference stuff like getting `rate limited` while doing an action, or getting `access denied` errors on operations. These will generally require further investigation by the team as a whole. Key things to check in these cases are the `CloudTrail` logs generated by the action runner. These actions will take place in `eu-west-1` in `analytical-platform-data-production`. Look for error messages being thrown by the role used by the runner, and then try and jump into the API calls. If the error is `Access Denied`, you can look at [the role that CaDeT uses](https://github.com/ministryofjustice/analytical-platform/main/terraform/aws/analytical-platform-data-production/create-a-derived-table/iam-roles.tf) to ensure the permission you're being denied by is added to the policy. In cases where `rate limits` are a factor, this is harder to resolve. We can try turning down the number of `threads` in our [profiles file](https://github.com/moj-analytical-services/create-a-derived-table/blob/main/.dbt/profiles.yml) for the relevant target, but this shouldn't be done unless there's no other options (such as asking the owner of the database that's spamming requests to tune down their usage).

**Pod just died 💀:**
All our github actions runners are now Ephemeral Action Runners, with monitoring via Kibana. If a pod fails part way through a run without really throwing any errors, odds are that the pod exploded due to exceeding a resource limit. Verify that this is the case (using the monitoring UI), and if it is, speak with the team about increasing the spec of the runner to accomodate.

## Contact Info

The Analytical Platform team can be contacted by [raising a support issue](https://github.com/ministryofjustice/data-platform-support/issues/new/choose). If you're reading this, you're probably a team member though, so just ask someone in the slack channel for help if there's an issue you get stuck on.
