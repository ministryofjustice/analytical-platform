<!-- markdownlint-disable -->
# Upgrade EKS from 1.20 to 1.21 (Development cluster)

The process follows the EKS documentation but adapted for our provisioning and config management tools i.e. Terraform, Helm and Flux. Some of the steps are manual (as per the AWS documents).

## References

NB: AWS documentation doesn't seem to be version specific so these reference may be made out of date

- [Previous upgrade](https://github.com/ministryofjustice/analytics-platform-infrastructure/blob/main/UPGRADE_EKS.md)
- [EKS ugrade](https://docs.aws.amazon.com/eks/latest/userguide/update-cluster.html)
- [VPC CNI](https://docs.aws.amazon.com/eks/latest/userguide/managing-vpc-cni.html#updating-vpc-cni-add-on)
- [CoreDNS](https://docs.aws.amazon.com/eks/latest/userguide/managing-coredns.html)
- [Node group](https://docs.aws.amazon.com/eks/latest/userguide/update-managed-node-group.html)
- [kube-proxy](https://docs.aws.amazon.com/eks/latest/userguide/managing-kube-proxy.html#updating-kube-proxy-add-on)

 ## Upgrade steps

NB these may differ for other version upgrades. YMMV.

1.  Check versions
2.  Check Pod Security Policy exists
3.  Check CoreDNS configuration
4.  Upgrade control plane (terraform)
5.  Upgrade node group (terraform)
6.  Autoscaler (helm/flux)
7.  Upgrade VPC CNI add on (terraform)
8.  CoreDNS (manual via kubectl)
9.  kube-proxy (manual via kubectl)
10. calico (helm)
## Notes

### General

The upgrade process is staged (EKS control plane first, then node group, then add ons). The cluster terraform has been update to parameterise the EKS, node group and VPC CNI versions to enable this process.

A simple nginx release (the one auto-generated by helm) was deployed with helm to check cluster functionality during the upgrade. Kubectl and the AWS console was used to for check progress and errors.

### Versions

It is worth checking the versions of all the components that need to be upgraded as well as the control plane and node versions; it is not recommended to run components more than two minor versions apart.

Compatibility tables are scattered throughout the AWS docs.

Kube-proxy and CoreDNS were at a lower release than the control plane. They were updated before moving on.

### Pod Security Policy

This existed already.

### CoreDNS configuration 

No action required.

### Upgrade EKS control plane

This is done by altering `cluster_version` in relevant account in `terraform.tfvars.json`.

- Previous value: 1.20
- Required value: 1.21

This took 20-30 minutes to complete.

### Upgrade node group

This is now done by altering `cluster_node_group_version` in relevant account in `terraform.tfvars.json`. Previously this was automatically set by terraform but altering the EKS version doesn't automatically update the node group.

Before this I attempted to recreate the node group by tainting the node group and re-running terraform. This failed, so reverted the taint and refactored the terraform to make the node group EKS version explicit.

- Previous value: 1.20
- Required value: 1.21

This took around 40 minutes. It spins up _a lot_ of extra nodes of the new version. Drains the existing nodes and then destroys them. Once there are no old nodes running it drains and destroys some of the new nodes to bring the cluster down to the desired node count.

### Autoscaler upgrade

This is done by altering the flux definition with appropriate helm chart version for the compatible application version.

- Previous value: Helm 9.9.2, App 1.20
- Required value Helm 9.10.4, App 1.21

### VPC-CNI plugin update

This is done by altering `cluster_vpc_cni_version` in relevant account in `terraform.tfvars.json`.

- Previous value: v1.8.0-eksbuild.1
- Required value: v1.9.0-eksbuild.1

This takes around 5-10 minutes.

### Upgrade CoreDNS

No change required from 1.20

### Upgrade kube-proxy

This is done by patching the kube-proxy daemon set manually with the new image version.

- Previous version: 602401143452.dkr.ecr.eu-west-1.amazonaws.com/eks/kube-proxy:v1.20.4-eksbuild.2
- Required version: 602401143452.dkr.ecr.eu-west-1.amazonaws.com/eks/kube-proxy:v1.21.2-eksbuild.2

### Calico (FAILED)

It's not clear from the documentation how to upgrade calico or even if it is necessary. On our clusters it is deployed via flux/helm using the AWS EKS helm charts. AWS do not recommended this. Instead they provide resource definitions to apply directly using kubectl apply.

After looking a the EKS helm chart releases I attempted to update to the latest version I could find but the redeployment failed with one of the daemonset containers on a crash backoff loop. Looking at the logs it appeared to be the exec health check was failing on setns.

On roll back the existing version of calico deployed successfully and works OK as far as I can tell.
