---
owner_slack: "#data-platform-notifications"
title: Data as a Product in Data Platform (PoC)
last_reviewed_on: 2023-09-27
review_in: 6 months
weight: 15
---

# <%= current_page.data.title %>

## Data as a Product: Vision

We want to design the Data Product management process around a user-facing API.
This API will allow users to register, update, and retrieve information about a Data Product.

The data sent to this API as part of registering or updating a data product will flow through standard Data Lake zones: Landing -> Raw -> Curated.

Each Data Product will require a [specification](https://github.com/ministryofjustice/data-platform-products/blob/main/data-products/_docs/product-specification.md) (still under refinement) alongside [other definitions](https://github.com/ministryofjustice/data-platform-products/tree/main/data-products#defining-a-data-product).

Data Products:

* May be multi-component, e.g. consisting of multiple tables
* Can consist of structured or unstructred data (or a combination)
* Are controlled and updated by Data Owners, Data Stewards, and Data Custodians.
* Include data metadata

> Implementation: The architecture will be built in the `data-platform-development` aws account, a sandbox account within [MoJ's Modernisation Platform](https://user-guide.modernisation-platform.service.justice.gov.uk/#modernisation-platform), which is deployed within the [modernisation-platform-environments](https://github.com/ministryofjustice/modernisation-platform-environments) repository


## Data Flow

Users with permissions to manage Data Products may register or amend data products.
Registration, and some categories of amendments will involve ingestion of data using our data API.
This API is backed by containerised lambdas that carry out each step of the event-driven data processing pipeline.
Data will first go to a ‘landing’ bucket ahead of data validation.
It is then moved to a ‘raw history’ bucket folder to retain a copy of the data.
The arrival of structured data in the raw history folder triggers a ‘curation’ lambda
which standardises the file type and data format before placing the data in the finalised ‘curated’ bucket folder.
This curated data is the data that is surfaced as the data product, and is registered to the aws Glue catalogue.

As part of the request to register or amend a Data Product, the API user must provide Data Product metadata.
This is processed, validated, and stored in a separate location to the data.
This then integrates with OpenMetadata to update the metadata in the catalogue
to reflect the changes made to the Data Product through this ingestion pipeline.

Each of the pipeline stages outputs standardised logging for audit and debugging purposes.
These logs are stored in another restricted-access logging bucket.

Each of the lambda images are stored in a central Elastic Container Registry on the MoJ Modernisation Platform.

![Data Product ingestion pipeline](../images/daap-data-flow-architecture.png "Data Product ingestion pipeline")


## Data Ingestion handling

Upon file transfer to the raw_data folder inside the data bucket an `aws.s3` `Object Created`
[EventBridge trigger is activated](https://github.com/ministryofjustice/modernisation-platform-environments/blob/main/terraform/environments/data-platform/triggers.tf).
This calls the [daap-athena-load lambda](https://github.com/ministryofjustice/data-platform/tree/main/containers/daap-athena-load),
which creates raw and curated tables in the Glue Catalog.

![Raw data processing via the daap-athena-load lambda](../images/daap-athena-load.png "Raw data processing via the daap-athena-load lambda")


### Assumptions and Risks

The following are the assumptions and risks. This will be mitigated eventually.

1. The data product language will be python which will enable us to run it using Glue.

2. The code provided by the data product will run without any errors. There is a risk that the code would be error prone. We need to think if there is any unit testing done to check the code provided.

3. Access control to various tools and users are out of scope for this work.


### Scope and Limitations

With this being used as a PoC it has quite a narrow scope and the following limitations:

* Raw data are a full-load snapshot at every upload.
* Raw data are csv format.
* Data producers cannot include transformations of the data.
* An extraction_timestamp - used as the partition - is the only column added to the data during the curation step (conversion to parquet). There is no SCD2.
* No schema changes can be made once a data product is created, users must create a new product

### Athena vs. Glue
Athena is cheaper than using Glue for compute as you are charged for amount of data scanned per query ($5 per TB).
Cost of lambda execution is also relatively inexpensive at $0.0000083 per second with 512 Mb memory.

Initial testing has shown Athena as compute to be a lot quicker than using a Glue Job for moderately sized data.
Further benchmarking is required as scale and complexity increases.

### Additional developments/ improvements

* **Schema**
    * (?) Improve schema inference.
    * Infer csv properties on uploads
        * e.g. delimiter, newline character, etc.
        * request these parameters at time of upload?
    * Read rows of csv data rather than a defined number of bytes of the file -
    e.g. [s3 select_object_content](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/s3/client/select_object_content.html) for 1000 lines
    * Allow data producer to define explicit table schema and use this when given rather than inferring.
    * Validate new data against existing schema for a given table.
    * Get partitions to create from existing partition folders or from the schema
        * (don’t just use MSCK REPAIR TABLE as this is less efficient)
    * Read xMb of 1st parquet file into memory when inferring schema. This is only necessary if something has gone wrong and an existing table is not registered in glue.

* **Transformation**
    * Allow data producer to push some config to perform simple transformations on the raw data. Things like lists of columns to include or simple aggregate sql statements. This could be injected into the sql used by the lambda to produce a simply transformed version of the raw data product. Defining simple is another task in itself.

* **Updates**
    * Deal with non-full snapshots - will need to know if has primary key.
    * Deal with CDC type loads, producing scd2 type database tables, where appropriate.

### Notes

#### Reporting tool

Athena is one example to provide route to access the data.
Various reporting tools can be attached later.

#### Use of aws services

##### API Gateway

API gateway is used to start the data ingestion process.

##### EventBridge

We are using EventBridge to kick off lambdas after each successive step of the data ingestion pipeline.

##### Lambda

We are using containerised lambdas for each of the processing steps in the data ingestion pipeline.
